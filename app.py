#å­˜ç®—åˆ†ç¦»,app.py è´Ÿè´£åœ¨çº¿é—®ç­”ï¼ˆå¿«é€Ÿå“åº”ï¼‰
#è¦å…ˆåœ¨cmdä¸­è¿è¡Œollama run qwen2.5:7b
#åœ¨ç»ˆç«¯è¾“å…¥ï¼šstreamlit run app.pyè¿›å…¥å‰ç«¯ç•Œé¢
import streamlit as st
import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 0. é…ç½®è·¯å¾„ (å¿…é¡»ä¸ build_db.py ä¿æŒä¸€è‡´) ---
DB_PATH = "./chroma_db_pro"

# --- 1. é¡µé¢åŸºç¡€è®¾ç½® ---
st.set_page_config(page_title="ææ–™çŸ¥è¯†åº“Pro", layout="wide")
st.title("ğŸ§ª ææ–™ç§‘å­¦çŸ¥è¯†åº“ç³»ç»Ÿ (Proç‰ˆ)")
st.caption("ğŸš€ å…¨åº“æ£€ç´¢æ¨¡å¼ | æ•°æ®å·²æŒä¹…åŒ–å­˜å‚¨")
st.markdown("---")

# --- 2. ä¾§è¾¹æ ï¼šçŠ¶æ€ç›‘æ§ä¸é…ç½® ---
with st.sidebar:
    st.header("ç³»ç»ŸçŠ¶æ€ monitor")

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if os.path.exists(DB_PATH):
        st.success(f"âœ… æœ¬åœ°çŸ¥è¯†åº“å·²è¿æ¥\n\nè·¯å¾„: `{DB_PATH}`")
    else:
        st.error("âŒ æœªæ‰¾åˆ°æ•°æ®åº“ï¼")
        st.warning("è¯·å…ˆè¿è¡Œ `build_db.py` æ„å»ºçŸ¥è¯†åº“ã€‚")

    st.markdown("---")
    st.header("æ¨¡å‹é…ç½®")
    # æŠŠåˆ—è¡¨é‡Œçš„ "llama3.2" æ¢æˆ "qwen2.5:7b"
    selected_model = st.selectbox("é€‰æ‹©æœ¬åœ°æ¨¡å‹", ["qwen2.5:7b"], index=0)

    # æ·»åŠ ä¸€ä¸ªæ¸…é™¤å†å²çš„æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©è®°å½•"):
        st.session_state.messages = []
        st.rerun()


# --- 3. æ ¸å¿ƒå‡½æ•° (åŠ è½½èµ„æº) ---
@st.cache_resource
def get_vector_db():
    """ç›´æ¥ä»ç¡¬ç›˜åŠ è½½å·²ç»å»ºå¥½çš„å‘é‡æ•°æ®åº“"""
    print(f"æ­£åœ¨åŠ è½½æ•°æ®åº“: {DB_PATH}")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    # persist_directory æŒ‡å‘ä¹‹å‰ build_db.py ç”Ÿæˆçš„æ–‡ä»¶å¤¹
    vector_db = Chroma(persist_directory=DB_PATH, embedding_function=embedding_model)
    return vector_db

#é»˜è®¤çš„ temperature (æ¸©åº¦) å¯èƒ½åé«˜ï¼Œå¯¼è‡´æ¨¡å‹æƒ³â€œå‘æŒ¥åˆ›æ„â€ã€‚åšç§‘ç ”é—®ç­”ï¼Œæˆ‘ä»¬éœ€è¦å®ƒåƒæœºå™¨äººä¸€æ ·æ­»æ¿ã€‚
#æŠŠæ¸©åº¦é™åˆ° 0ï¼Œå¼ºåˆ¶å®ƒå®Œå…¨åŸºäºäº‹å®ã€‚
@st.cache_resource
def get_llm(model_name):
    # å°† temperature æ”¹ä¸º 0 (æœ€ä¸¥è°¨æ¨¡å¼)
    return OllamaLLM(model=model_name, temperature=0)


# --- 4. ä¸»ç•Œé¢é€»è¾‘ ---

# åˆå§‹åŒ–èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å°±ç»ª
if not os.path.exists(DB_PATH):
    st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ï¼è¯·å…ˆåœ¨ PyCharm ä¸­è¿è¡Œ `build_db.py` æ¥æ„å»ºä½ çš„ææ–™æ•°æ®åº“ï¼Œç„¶ååˆ·æ–°æœ¬é¡µé¢ã€‚")
    st.stop()  # åœæ­¢å¾€ä¸‹æ‰§è¡Œ

# åŠ è½½æ•°æ®åº“ (åˆ©ç”¨ç¼“å­˜ï¼Œåªä¼šåŠ è½½ä¸€æ¬¡)
vector_db = get_vector_db()

# æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- 5. å¤„ç†ç”¨æˆ·æé—® ---
if prompt := st.chat_input("è¯·è¾“å…¥å…³äºæ–‡çŒ®çš„é—®é¢˜ (å°†æ£€ç´¢ data ç›®å½•ä¸‹çš„æ‰€æœ‰ PDF)..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").markdown(prompt)

    # 2. AI å›ç­”
    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        msg_placeholder.markdown("ğŸ” æ­£åœ¨å…¨åº“æ£€ç´¢...")

        try:
            # A. æ£€ç´¢å™¨ï¼šæŸ¥æ‰¾æœ€ç›¸å…³çš„ 4 ä¸ªç‰‡æ®µ
            retriever = vector_db.as_retriever(search_kwargs={"k": 10})
            llm = get_llm(selected_model)

            # B. æç¤ºè¯æ¨¡æ¿ (ä¼˜åŒ–ç‰ˆ)
            # --- é’ˆå¯¹ PDF ä¹±ç ä¼˜åŒ–çš„ Prompt ---
            template = """
            ä½ æ˜¯ä¸€ä¸ªææ–™ç§‘å­¦ä¸“å®¶ã€‚è¯·æ ¹æ®ä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚

            âš ï¸ä»¥æ­¤ä¸ºå‡†ï¼š
            1. **è‡ªåŠ¨çº é”™**ï¼šå‚è€ƒèµ„æ–™å¯èƒ½åŒ…å« PDF è¯†åˆ«é”™è¯¯ï¼ˆä¾‹å¦‚å•è¯ç²˜è¿ "Wintercalates" -> "W intercalates"ï¼‰ï¼Œè¯·å°è¯•ç†è§£å…¶çœŸå®å«ä¹‰ã€‚
            2. **æå–äº‹å®**ï¼šé‡ç‚¹å¯»æ‰¾ä¸é—®é¢˜ç›¸å…³çš„**åŒ–å­¦å¼ã€æ•°å­—ã€ä½ç½®å…³ç³»**ã€‚
            3. **è¯­è¨€è¦æ±‚**ï¼šå°½é‡ç”¨ä¸­æ–‡å›ç­”ï¼Œä½†ä¿ç•™è‹±æ–‡ä¸“æœ‰åè¯ã€‚
            4. å¦‚æœå®åœ¨æ‰¾ä¸åˆ°ä»»ä½•ç›¸å…³çº¿ç´¢ï¼Œå†è¯´â€œæœªæ‰¾åˆ°â€ã€‚
            5. å‚è€ƒèµ„æ–™ä¿ç•™äº†åŸå§‹ PDF çš„**è§†è§‰å¸ƒå±€**ã€‚
            6. è¡¨æ ¼æ˜¯é€šè¿‡**ç©ºæ ¼å’Œæ¢è¡Œ**å¯¹é½çš„ã€‚

            ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
            {context}

            ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
            {question}
            """
            rag_prompt = ChatPromptTemplate.from_template(template)


            # C. æ„å»º RAG æµæ°´çº¿
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)


            rag_chain = (
                    {"context": retriever | format_docs, "question": RunnablePassthrough()}
                    | rag_prompt
                    | llm
                    | StrOutputParser()
            )

            # D. æ‰§è¡Œæ£€ç´¢ä¸ç”Ÿæˆ
            # å…ˆæ£€ç´¢ä¸€éï¼Œä¸ºäº†åœ¨ç•Œé¢ä¸Šå±•ç¤ºæ¥æº (Debugç”¨)
            retrieved_docs = retriever.invoke(prompt)

            # ç”Ÿæˆå›ç­”
            response = rag_chain.invoke(prompt)
            msg_placeholder.markdown(response)

            # ä¿å­˜å›ç­”åˆ°å†å²
            st.session_state.messages.append({"role": "assistant", "content": response})

            # E. æ ¸å¿ƒäº®ç‚¹ï¼šå±•ç¤ºæ£€ç´¢æ¥æº (è®ºæ–‡åŠ åˆ†é¡¹)
            with st.expander("ğŸ“š æŸ¥çœ‹æ¥æºæ–‡æ¡£ (Evidence)"):
                for i, doc in enumerate(retrieved_docs):
                    # è·å–æ–‡ä»¶å (source metadata)
                    source_path = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = os.path.basename(source_path)  # åªæ˜¾ç¤ºæ–‡ä»¶åï¼Œä¸æ˜¾ç¤ºé•¿è·¯å¾„

                    st.markdown(f"**æ¥æº {i + 1}:** `{file_name}`")
                    st.caption(f"å†…å®¹æ‘˜è¦: {doc.page_content[:500]}...")  # åªæ˜¾ç¤ºå‰500å­—
                    st.markdown("---")

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")